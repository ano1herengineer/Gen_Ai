{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to LSTM RNN\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) designed to overcome the limitations of traditional RNNs, particularly in handling long-term dependencies. While RNNs are capable of processing sequential data by maintaining a hidden state that captures information from previous time steps, they struggle with the vanishing gradient problem. This issue arises during backpropagation, where gradients can become very small, causing the network to stop learning effectively over long sequences.\n",
    "\n",
    "LSTMs address this problem by incorporating a more sophisticated architecture that includes mechanisms to control the flow of information. These mechanisms, known as gates, allow LSTMs to retain and utilize information over extended periods, making them highly effective for tasks that require understanding long-term dependencies. By mitigating the vanishing gradient problem, LSTMs have become a popular choice for various applications, including natural language processing, time series forecasting, and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problem with RNN\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are designed to handle sequential data by maintaining a hidden state that captures information from previous time steps. However, RNNs struggle with long-term dependencies due to the vanishing gradient problem. During backpropagation, gradients can become very small, causing the network to stop learning effectively. This makes it difficult for RNNs to retain information over long sequences.\n",
    "\n",
    "### 2. Why LSTM RNN\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks were introduced to address the limitations of standard RNNs. LSTMs are capable of learning long-term dependencies by using a more complex architecture that includes mechanisms to control the flow of information. This helps in mitigating the vanishing gradient problem, allowing the network to retain information over longer periods.\n",
    "\n",
    "### 3. How LSTM RNN Works\n",
    "\n",
    "LSTMs use a combination of long-term memory and short-term memory to manage information flow.\n",
    "\n",
    "#### a) Long Term Memory\n",
    "\n",
    "Long-term memory in LSTMs is maintained through a cell state that runs through the entire sequence. This cell state acts as a conveyor belt, allowing information to flow unchanged unless explicitly modified by gates.\n",
    "\n",
    "#### b) Short Term Memory\n",
    "\n",
    "Short-term memory is managed through hidden states that capture information from the current time step and are updated at each step.\n",
    "\n",
    "### 4. LSTM Architecture\n",
    "\n",
    "LSTM networks consist of a series of cells, each containing three main gates:\n",
    "\n",
    "- **Forget Gate**: Decides what information to discard from the cell state.\n",
    "- **Input Gate**: Determines which new information to add to the cell state.\n",
    "- **Output Gate**: Controls what information to output based on the cell state and hidden state.\n",
    "\n",
    "### 5. Working of LSTM RNN\n",
    "\n",
    "1. **Forget Gate**: The forget gate takes the previous hidden state and the current input to produce a value between 0 and 1 for each number in the cell state. A value of 0 means \"completely forget\" and a value of 1 means \"completely keep.\"\n",
    "\n",
    "    ```python\n",
    "    f_t = sigmoid(W_f * [h_{t-1}, x_t] + b_f)\n",
    "    ```\n",
    "\n",
    "2. **Input Gate**: The input gate decides which values to update. It consists of two parts: a sigmoid layer that decides which values to update and a tanh layer that creates a vector of new candidate values.\n",
    "\n",
    "    ```python\n",
    "    i_t = sigmoid(W_i * [h_{t-1}, x_t] + b_i)\n",
    "    \\tilde{C}_t = tanh(W_C * [h_{t-1}, x_t] + b_C)\n",
    "    ```\n",
    "\n",
    "3. **Update Cell State**: The cell state is updated by combining the old cell state, scaled by the forget gate, and the new candidate values, scaled by the input gate.\n",
    "\n",
    "    ```python\n",
    "    C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t\n",
    "    ```\n",
    "\n",
    "4. **Output Gate**: The output gate decides what the next hidden state should be. It is based on the updated cell state and the current input.\n",
    "\n",
    "    ```python\n",
    "    o_t = sigmoid(W_o * [h_{t-1}, x_t] + b_o)\n",
    "    h_t = o_t * tanh(C_t)\n",
    "    ```\n",
    "\n",
    "By using these gates, LSTMs can effectively manage long-term dependencies and mitigate the vanishing gradient problem, making them suitable for tasks that require learning from long sequences of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Operation\n",
    "\n",
    "LSTM networks use a series of gates to control the flow of information through the network. These gates perform various operations to manage long-term and short-term memory effectively. Hereâ€™s a breakdown of the key operations involved in an LSTM cell:\n",
    "\n",
    "#### 1. Pointwise Operations\n",
    "\n",
    "Pointwise operations are element-wise operations applied to vectors or matrices. In LSTMs, these operations include:\n",
    "\n",
    "- **Sigmoid Activation**: Used in gates to produce values between 0 and 1, indicating how much information to keep or discard.\n",
    "    ```python\n",
    "    sigmoid(x) = 1 / (1 + exp(-x))\n",
    "    ```\n",
    "\n",
    "- **Tanh Activation**: Used to create new candidate values, producing values between -1 and 1.\n",
    "    ```python\n",
    "    tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "    ```\n",
    "\n",
    "- **Element-wise Multiplication**: Used to update the cell state by combining the forget gate output and the previous cell state, as well as the input gate output and the new candidate values.\n",
    "    ```python\n",
    "    C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t\n",
    "    ```\n",
    "\n",
    "#### 2. Vector Transfer\n",
    "\n",
    "Vector transfer involves passing vectors through various layers and operations within the LSTM cell. Key vectors include:\n",
    "\n",
    "- **Cell State (C_t)**: The long-term memory of the LSTM, which is updated at each time step.\n",
    "- **Hidden State (h_t)**: The short-term memory, which is also the output of the LSTM cell at each time step.\n",
    "\n",
    "#### 3. Concatenate\n",
    "\n",
    "Concatenation is used to combine the previous hidden state and the current input before passing them through the gates. This combined vector is used to compute the gate activations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = concatenate([h_{t-1}, x_t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 4. Copy\n",
    "\n",
    "Copy operations are used to replicate vectors for use in multiple operations. For example, the cell state is copied and modified by the forget gate and the input gate.\n",
    "\n",
    "### LSTM Cell Operations\n",
    "\n",
    "1. Forget Fate\n",
    "2. Input Gate \n",
    "3. Update Output Cell(Candidate Memory)\n",
    "4. Output Gate \n",
    "\n",
    "\n",
    "By using these operations, LSTMs effectively manage long-term dependencies and mitigate the vanishing gradient problem, making them suitable for tasks that require learning from long sequences of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Forget Gate**:\n",
    "    ```python\n",
    "    f_t = sigmoid(W_f * [h_{t-1}, x_t] + b_f)\n",
    "    ```\n",
    "    - **f_t**: Forget gate activation vector at time step t. It determines which information to discard from the cell state.\n",
    "    - **sigmoid**: Sigmoid activation function, which outputs values between 0 and 1.\n",
    "    - **W_f**: Weight matrix for the forget gate.\n",
    "    - **h_{t-1}**: Hidden state from the previous time step (t-1).\n",
    "    - **x_t**: Input vector at the current time step t.\n",
    "    - **b_f**: Bias vector for the forget gate.\n",
    "\n",
    "2. **Input Gate**:\n",
    "    ```python\n",
    "    i_t = sigmoid(W_i * [h_{t-1}, x_t] + b_i)\n",
    "    \\tilde{C}_t = tanh(W_C * [h_{t-1}, x_t] + b_C)\n",
    "    ```\n",
    "    - **i_t**: Input gate activation vector at time step t. It determines which new information to add to the cell state.\n",
    "    - **sigmoid**: Sigmoid activation function, which outputs values between 0 and 1.\n",
    "    - **W_i**: Weight matrix for the input gate.\n",
    "    - **b_i**: Bias vector for the input gate.\n",
    "    - **\\tilde{C}_t**: Candidate cell state vector at time step t, created by the tanh layer.\n",
    "    - **tanh**: Hyperbolic tangent activation function, which outputs values between -1 and 1.\n",
    "    - **W_C**: Weight matrix for the candidate cell state.\n",
    "    - **b_C**: Bias vector for the candidate cell state.\n",
    "\n",
    "3. **Update Cell State**:\n",
    "    ```python\n",
    "    C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t\n",
    "    ```\n",
    "    - **C_t**: Cell state vector at time step t.\n",
    "    - **f_t**: Forget gate activation vector at time step t.\n",
    "    - **C_{t-1}**: Cell state vector from the previous time step (t-1).\n",
    "    - **i_t**: Input gate activation vector at time step t.\n",
    "    - **\\tilde{C}_t**: Candidate cell state vector at time step t.\n",
    "\n",
    "4. **Output Gate**:\n",
    "    ```python\n",
    "    o_t = sigmoid(W_o * [h_{t-1}, x_t] + b_o)\n",
    "    h_t = o_t * tanh(C_t)\n",
    "    ```\n",
    "    - **o_t**: Output gate activation vector at time step t. It determines what the next hidden state should be.\n",
    "    - **sigmoid**: Sigmoid activation function, which outputs values between 0 and 1.\n",
    "    - **W_o**: Weight matrix for the output gate.\n",
    "    - **h_{t-1}**: Hidden state from the previous time step (t-1).\n",
    "    - **x_t**: Input vector at the current time step t.\n",
    "    - **b_o**: Bias vector for the output gate.\n",
    "    - **h_t**: Hidden state vector at time step t.\n",
    "    - **tanh**: Hyperbolic tangent activation function, which outputs values between -1 and 1.\n",
    "    - **C_t**: Cell state vector at time step t.\n",
    "\n",
    "These terms collectively help the LSTM network manage long-term dependencies by controlling the flow of information through the cell state and hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM (Long Short-Term Memory) networks were introduced by Sepp Hochreiter and JÃ¼rgen Schmidhuber in 1997 to address the limitations of traditional RNNs, particularly the vanishing gradient problem. Over time, several variants of LSTM networks have been proposed to improve their performance and adapt them to different tasks. Here are some notable LSTM variants:\n",
    "\n",
    "### 1. Vanilla LSTM\n",
    "The original LSTM architecture introduced by Hochreiter and Schmidhuber includes the following components:\n",
    "- **Forget Gate**: Decides what information to discard from the cell state.\n",
    "- **Input Gate**: Determines which new information to add to the cell state.\n",
    "- **Output Gate**: Controls what information to output based on the cell state and hidden state.\n",
    "\n",
    "### 2. Peephole LSTM\n",
    "Introduced by Felix Gers and JÃ¼rgen Schmidhuber in 2000, Peephole LSTMs allow the gates to have access to the cell state. This means that the input, forget, and output gates can look at the cell state in addition to the hidden state and input.\n",
    "\n",
    "- **Peephole Connections**: Connections from the cell state to the gates.\n",
    "    ```python\n",
    "    f_t = sigmoid(W_f * [h_{t-1}, x_t, C_{t-1}] + b_f)\n",
    "    i_t = sigmoid(W_i * [h_{t-1}, x_t, C_{t-1}] + b_i)\n",
    "    o_t = sigmoid(W_o * [h_{t-1}, x_t, C_t] + b_o)\n",
    "    ```\n",
    "\n",
    "### 3. Gated Recurrent Unit (GRU)\n",
    "Introduced by Kyunghyun Cho et al. in 2014, GRUs are a simplified version of LSTMs that combine the forget and input gates into a single update gate. GRUs have fewer parameters and are computationally more efficient.\n",
    "\n",
    "- **Update Gate**: Combines the forget and input gates.\n",
    "- **Reset Gate**: Determines how much of the previous hidden state to forget.\n",
    "\n",
    "    ```python\n",
    "    z_t = sigmoid(W_z * [h_{t-1}, x_t] + b_z)\n",
    "    r_t = sigmoid(W_r * [h_{t-1}, x_t] + b_r)\n",
    "    \\tilde{h}_t = tanh(W_h * [r_t * h_{t-1}, x_t] + b_h)\n",
    "    h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t\n",
    "    ```\n",
    "\n",
    "### 4. Bidirectional LSTM\n",
    "Bidirectional LSTMs process the input sequence in both forward and backward directions, allowing the network to have information from both past and future states. This is particularly useful for tasks where context from both directions is important.\n",
    "\n",
    "- **Forward LSTM**: Processes the sequence from start to end.\n",
    "- **Backward LSTM**: Processes the sequence from end to start.\n",
    "\n",
    "    ```python\n",
    "    h_t_forward = LSTM_forward(x_t)\n",
    "    h_t_backward = LSTM_backward(x_t)\n",
    "    h_t = concatenate([h_t_forward, h_t_backward])\n",
    "    ```\n",
    "\n",
    "### 5. Stacked LSTM\n",
    "Stacked LSTMs consist of multiple LSTM layers stacked on top of each other. This allows the network to learn more complex representations by passing the hidden states from one layer to the next.\n",
    "\n",
    "- **Multiple LSTM Layers**: Each layer processes the hidden states from the previous layer.\n",
    "\n",
    "    ```python\n",
    "    h_t_layer1 = LSTM_layer1(x_t)\n",
    "    h_t_layer2 = LSTM_layer2(h_t_layer1)\n",
    "    ```\n",
    "\n",
    "### 6. Attention Mechanism\n",
    "While not a variant of LSTM itself, the attention mechanism can be combined with LSTMs to improve their performance on tasks like machine translation and text summarization. Attention allows the network to focus on specific parts of the input sequence when making predictions.\n",
    "\n",
    "- **Attention Weights**: Determine the importance of each part of the input sequence.\n",
    "- **Context Vector**: Weighted sum of the input sequence based on attention weights.\n",
    "\n",
    "    ```python\n",
    "    attention_weights = softmax(score(h_t, encoder_outputs))\n",
    "    context_vector = sum(attention_weights * encoder_outputs)\n",
    "    ```\n",
    "\n",
    "These variants and extensions of LSTM networks have been developed to address specific challenges and improve performance on various tasks. Each variant has its own strengths and is suited for different types of sequential data and applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
