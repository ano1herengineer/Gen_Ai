{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
    "\n",
    "### Key Components of NLP:\n",
    "1. **Tokenization**: Breaking down text into smaller units, such as words or sentences.\n",
    "2. **Part-of-Speech Tagging**: Identifying the grammatical parts of speech in a sentence (e.g., nouns, verbs, adjectives).\n",
    "3. **Named Entity Recognition (NER)**: Identifying and classifying entities in text into predefined categories such as names of people, organizations, locations, etc.\n",
    "4. **Sentiment Analysis**: Determining the sentiment or emotion expressed in a piece of text.\n",
    "5. **Machine Translation**: Translating text from one language to another.\n",
    "6. **Text Summarization**: Creating a concise summary of a longer text.\n",
    "7. **Speech Recognition**: Converting spoken language into text.\n",
    "8. **Text Generation**: Generating human-like text based on a given input.\n",
    "\n",
    "### Applications of NLP:\n",
    "- **Chatbots and Virtual Assistants**: Enabling conversational agents like Siri, Alexa, and Google Assistant.\n",
    "- **Sentiment Analysis**: Analyzing customer reviews, social media posts, and feedback.\n",
    "- **Machine Translation**: Translating text between languages using tools like Google Translate.\n",
    "- **Information Retrieval**: Enhancing search engines to understand and retrieve relevant information.\n",
    "- **Text Summarization**: Summarizing articles, documents, and reports.\n",
    "\n",
    "### Libraries and Tools:\n",
    "- **NLTK (Natural Language Toolkit)**: A comprehensive library for building NLP programs in Python.\n",
    "- **spaCy**: An open-source library for advanced NLP in Python.\n",
    "- **Transformers (Hugging Face)**: A library for state-of-the-art NLP models like BERT, GPT, etc.\n",
    "- **Gensim**: A library for topic modeling and document similarity analysis.\n",
    "\n",
    "NLP combines computational linguistics, machine learning, and deep learning techniques to process and analyze large amounts of natural language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, sentences, or even subwords. Tokenization is a fundamental step in many NLP tasks as it helps in understanding the structure and meaning of the text.\n",
    "\n",
    "### NLTK Tokenization Methods\n",
    "NLTK (Natural Language Toolkit) provides several methods for tokenization:\n",
    "\n",
    "1. **`sent_tokenize`**:\n",
    "   - **Purpose**: Splits a text into sentences.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from nltk.tokenize import sent_tokenize\n",
    "     text = \"Hello world. This is a test.\"\n",
    "     sentences = sent_tokenize(text)\n",
    "     print(sentences)  # Output: ['Hello world.', 'This is a test.']\n",
    "     ```\n",
    "\n",
    "2. **`word_tokenize`**:\n",
    "   - **Purpose**: Splits a sentence into words.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from nltk.tokenize import word_tokenize\n",
    "     sentence = \"Hello world.\"\n",
    "     words = word_tokenize(sentence)\n",
    "     print(words)  # Output: ['Hello', 'world', '.']\n",
    "     ```\n",
    "\n",
    "3. **`wordpunct_tokenize`**:\n",
    "   - **Purpose**: Splits a sentence into words and punctuation.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from nltk.tokenize import wordpunct_tokenize\n",
    "     sentence = \"Hello world!\"\n",
    "     tokens = wordpunct_tokenize(sentence)\n",
    "     print(tokens)  # Output: ['Hello', 'world', '!']\n",
    "     ```\n",
    "\n",
    "4. **`TreebankWordTokenizer`**:\n",
    "   - **Purpose**: Uses the Penn Treebank tokenizer to split a sentence into words. It handles punctuation and contractions more accurately.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from nltk.tokenize import TreebankWordTokenizer\n",
    "     tokenizer = TreebankWordTokenizer()\n",
    "     sentence = \"They'll save and invest.\"\n",
    "     tokens = tokenizer.tokenize(sentence)\n",
    "     print(tokens)  # Output: ['They', \"'ll\", 'save', 'and', 'invest', '.']\n",
    "     ```\n",
    "\n",
    "### Summary\n",
    "- **`sent_tokenize`**: Splits text into sentences.\n",
    "- **`word_tokenize`**: Splits sentences into words.\n",
    "- **`wordpunct_tokenize`**: Splits sentences into words and punctuation.\n",
    "- **`TreebankWordTokenizer`**: Splits sentences into words using the Penn Treebank tokenizer, handling punctuation and contractions accurately.\n",
    "\n",
    "These tokenization methods are essential for preprocessing text data in various NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming in NLTK\n",
    "Stemming is the process of reducing words to their base or root form. The goal is to remove morphological affixes from words, leaving only the word stem. This is useful in NLP tasks to treat different forms of a word as the same term, which can improve the performance of text analysis algorithms.\n",
    "\n",
    "### NLTK Stemming Classes\n",
    "NLTK provides several stemming classes, each implementing different stemming algorithms:\n",
    "\n",
    "1. **Porter Stemmer**:\n",
    "   - **Description**: One of the oldest and most widely used stemming algorithms. It uses a series of rules to iteratively reduce words to their stems.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from nltk.stem import PorterStemmer\n",
    "     stemmer = PorterStemmer()\n",
    "     words = [\"running\", \"jumps\", \"easily\", \"fairly\"]\n",
    "     stems = [stemmer.stem(word) for word in words]\n",
    "     print(stems)  # Output: ['run', 'jump', 'easili', 'fairli']\n",
    "     ```\n",
    "\n",
    "2. **RegexpStemmer**:\n",
    "   - **Description**: Uses regular expressions to remove affixes from words. It is more customizable but less sophisticated than other stemmers.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from nltk.stem import RegexpStemmer\n",
    "     stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "     words = [\"running\", \"jumps\", \"easily\", \"fairly\"]\n",
    "     stems = [stemmer.stem(word) for word in words]\n",
    "     print(stems)  # Output: ['runn', 'jump', 'easili', 'fairli']\n",
    "     ```\n",
    "\n",
    "3. **Snowball Stemmer**:\n",
    "   - **Description**: An improvement over the Porter Stemmer, supporting multiple languages. It is more aggressive and accurate in reducing words to their stems.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from nltk.stem import SnowballStemmer\n",
    "     stemmer = SnowballStemmer(\"english\")\n",
    "     words = [\"running\", \"jumps\", \"easily\", \"fairly\"]\n",
    "     stems = [stemmer.stem(word) for word in words]\n",
    "     print(stems)  # Output: ['run', 'jump', 'easili', 'fair']\n",
    "     ```\n",
    "\n",
    "### Summary\n",
    "- **Porter Stemmer**: Uses a series of rules to iteratively reduce words to their stems. It is simple and widely used.\n",
    "- **RegexpStemmer**: Uses regular expressions to remove affixes. It is customizable but less sophisticated.\n",
    "- **Snowball Stemmer**: An improvement over the Porter Stemmer, supporting multiple languages and providing more accurate stemming.\n",
    "\n",
    "These stemming algorithms help in normalizing text data, which is crucial for various NLP tasks such as text classification, information retrieval, and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization in NLTK\n",
    "Lemmatization is the process of reducing words to their base or dictionary form, known as a lemma. Unlike stemming, which simply cuts off prefixes or suffixes, lemmatization considers the context and morphological analysis of the words. This results in more accurate and meaningful base forms.\n",
    "\n",
    "### NLTK Lemmatization Classes\n",
    "NLTK provides the `WordNetLemmatizer` class for lemmatization, which uses the WordNet lexical database.\n",
    "\n",
    "1. **WordNet Lemmatizer**:\n",
    "   - **Description**: Uses the WordNet database to find the lemma of a word. It requires the part of speech (POS) tag to perform accurate lemmatization.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     from nltk.stem import WordNetLemmatizer\n",
    "     from nltk.corpus import wordnet\n",
    "\n",
    "     # Initialize the lemmatizer\n",
    "     lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "     # Function to get POS tag for lemmatization\n",
    "     def get_wordnet_pos(word):\n",
    "         \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "         tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "         tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "         return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "     words = [\"running\", \"jumps\", \"easily\", \"fairly\"]\n",
    "     lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "     print(lemmas)  # Output: ['run', 'jump', 'easily', 'fairly']\n",
    "     ```\n",
    "\n",
    "### Summary\n",
    "- **Lemmatization**: Reduces words to their base or dictionary form (lemma) by considering the context and morphological analysis.\n",
    "- **WordNet Lemmatizer**: Uses the WordNet lexical database to find the lemma of a word. It requires the part of speech (POS) tag for accurate lemmatization.\n",
    "\n",
    "### Comparison with Stemming\n",
    "- **Stemming**: Cuts off prefixes or suffixes to reduce words to their base form. It is faster but less accurate.\n",
    "- **Lemmatization**: Considers the context and morphological analysis to reduce words to their dictionary form. It is more accurate but slower.\n",
    "\n",
    "Lemmatization is particularly useful in NLP tasks where understanding the context and meaning of words is crucial, such as text analysis, information retrieval, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing with Stopwords and Stemming\n",
    "\n",
    "Text preprocessing is a crucial step in NLP tasks. It involves cleaning and preparing text data for analysis. Common preprocessing steps include removing stopwords and applying stemming.\n",
    "\n",
    "### Steps:\n",
    "1. **Remove Stopwords**: Stopwords are common words (e.g., \"and\", \"the\", \"is\") that are often removed from text because they do not carry significant meaning.\n",
    "2. **Apply Stemming**: Reduce words to their base or root form.\n",
    "\n",
    "### Code Implementation\n",
    "\n",
    "#### 1. Apply Stopwords and Filter, then Apply Porter Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a simple example to demonstrate text preprocessing.\"\n",
    "\n",
    "# Tokenize text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords and apply stemming\n",
    "filtered_stemmed_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_stemmed_words)  # Output: ['thi', 'simpl', 'exampl', 'demonstr', 'text', 'preprocess', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 2. Apply Stopwords and Filter, then Apply Snowball Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a simple example to demonstrate text preprocessing.\"\n",
    "\n",
    "# Tokenize text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords and apply stemming\n",
    "filtered_stemmed_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_stemmed_words)  # Output: ['thi', 'simpl', 'exampl', 'demonstr', 'text', 'preprocess', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Summary\n",
    "- **Stopwords Removal**: Removes common words that do not carry significant meaning.\n",
    "- **Porter Stemming**: Reduces words to their base form using the Porter stemming algorithm.\n",
    "- **Snowball Stemming**: Reduces words to their base form using the Snowball stemming algorithm, which is more aggressive and accurate.\n",
    "\n",
    "These preprocessing steps help in normalizing text data, making it more suitable for various NLP tasks such as text classification, sentiment analysis, and information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech (POS) Tagging using NLTK\n",
    "\n",
    "Part of Speech (POS) tagging is the process of assigning a part of speech to each word in a sentence. Common POS tags include nouns, verbs, adjectives, adverbs, etc. POS tagging is essential for understanding the grammatical structure of a sentence and is a fundamental step in many NLP tasks.\n",
    "\n",
    "### Steps to Perform POS Tagging using NLTK\n",
    "1. **Tokenize the Sentence**: Split the sentence into words.\n",
    "2. **Tag the Tokens**: Assign a POS tag to each token.\n",
    "\n",
    "### Code Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a simple example to demonstrate POS tagging.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "print(pos_tags)\n",
    "# Output: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('simple', 'JJ'), ('example', 'NN'), ('to', 'TO'), ('demonstrate', 'VB'), ('POS', 'NNP'), ('tagging', 'NN'), ('.', '.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation\n",
    "1. **Download NLTK Data**: Ensure that the necessary NLTK data files are downloaded.\n",
    "2. **Tokenize the Sentence**: Use `nltk.word_tokenize` to split the sentence into words.\n",
    "3. **POS Tagging**: Use `nltk.pos_tag` to assign POS tags to each token.\n",
    "\n",
    "### POS Tags\n",
    "Here are some common POS tags used by NLTK:\n",
    "- **NN**: Noun, singular or mass\n",
    "- **NNS**: Noun, plural\n",
    "- **NNP**: Proper noun, singular\n",
    "- **NNPS**: Proper noun, plural\n",
    "- **VB**: Verb, base form\n",
    "- **VBD**: Verb, past tense\n",
    "- **VBG**: Verb, gerund or present participle\n",
    "- **VBN**: Verb, past participle\n",
    "- **VBP**: Verb, non-3rd person singular present\n",
    "- **VBZ**: Verb, 3rd person singular present\n",
    "- **JJ**: Adjective\n",
    "- **JJR**: Adjective, comparative\n",
    "- **JJS**: Adjective, superlative\n",
    "- **RB**: Adverb\n",
    "- **RBR**: Adverb, comparative\n",
    "- **RBS**: Adverb, superlative\n",
    "- **DT**: Determiner\n",
    "- **IN**: Preposition or subordinating conjunction\n",
    "- **TO**: to\n",
    "\n",
    "### Summary\n",
    "POS tagging is a crucial step in NLP that helps in understanding the grammatical structure of a sentence. NLTK provides easy-to-use functions for tokenizing text and assigning POS tags, making it a powerful tool for text analysis and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) using NLTK\n",
    "\n",
    "Named Entity Recognition (NER) is the process of identifying and classifying named entities in text into predefined categories such as names of persons, organizations, locations, dates, etc. NER is a crucial step in many NLP tasks, including information extraction, question answering, and text summarization.\n",
    "\n",
    "### Steps to Perform NER using NLTK\n",
    "1. **Tokenize the Sentence**: Split the sentence into words.\n",
    "2. **POS Tagging**: Assign a part of speech to each word.\n",
    "3. **Chunking**: Group the tagged words into named entities.\n",
    "\n",
    "### Code Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text\n",
    "text = \"Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Perform Named Entity Recognition\n",
    "named_entities = nltk.ne_chunk(pos_tags)\n",
    "\n",
    "print(named_entities)\n",
    "# Output: (S\n",
    "#           (PERSON Barack/NNP)\n",
    "#           (PERSON Obama/NNP)\n",
    "#           was/VBD\n",
    "#           born/VBN\n",
    "#           on/IN\n",
    "#           August/NNP\n",
    "#           4/CD\n",
    "#           ,/,\n",
    "#           1961/CD\n",
    "#           ,/,\n",
    "#           in/IN\n",
    "#           (GPE Honolulu/NNP)\n",
    "#           ,/,\n",
    "#           (GPE Hawaii/NNP)\n",
    "#           ./.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Explanation\n",
    "1. **Download NLTK Data**: Ensure that the necessary NLTK data files are downloaded.\n",
    "2. **Tokenize the Sentence**: Use `nltk.word_tokenize` to split the sentence into words.\n",
    "3. **POS Tagging**: Use `nltk.pos_tag` to assign POS tags to each token.\n",
    "4. **Named Entity Recognition**: Use `nltk.ne_chunk` to identify and classify named entities in the text.\n",
    "\n",
    "### Named Entity Types\n",
    "Here are some common named entity types recognized by NLTK:\n",
    "- **PERSON**: People, including fictional.\n",
    "- **ORGANIZATION**: Companies, agencies, institutions, etc.\n",
    "- **GPE**: Geopolitical entities, such as countries, cities, states.\n",
    "- **LOCATION**: Non-GPE locations, mountain ranges, bodies of water.\n",
    "- **DATE**: Absolute or relative dates or periods.\n",
    "- **TIME**: Times smaller than a day.\n",
    "- **MONEY**: Monetary values, including currency.\n",
    "- **PERCENT**: Percentage (including \"%\").\n",
    "- **FACILITY**: Buildings, airports, highways, bridges, etc.\n",
    "\n",
    "### Summary\n",
    "Named Entity Recognition (NER) is a crucial step in NLP for identifying and classifying named entities in text. NLTK provides easy-to-use functions for tokenizing text, POS tagging, and performing NER, making it a powerful tool for text analysis and information extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the Sentiment Analysis Process\n",
    "\n",
    "Sentiment analysis is the process of determining the emotional tone behind a series of words, used to gain an understanding of the attitudes, opinions, and emotions expressed within an online mention. Here is a 5-step process for performing sentiment analysis:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Data Collection**:\n",
    "   - **Dataset**: Gather text data from various sources such as social media, reviews, surveys, etc. Ensure the data is relevant to the analysis.\n",
    "\n",
    "2. **Text Preprocessing (Part 1)**:\n",
    "   - **Tokenization**: Split the text into individual words or tokens.\n",
    "   - **Lowercase Words**: Convert all text to lowercase to ensure uniformity.\n",
    "   - **Regular Expressions**: Use regex to remove unwanted characters, punctuation, and special symbols.\n",
    "\n",
    "3. **Text Preprocessing (Part 2)**:\n",
    "   - **Stemming**: Reduce words to their base or root form (e.g., \"running\" to \"run\").\n",
    "   - **Lemmatization**: Reduce words to their dictionary form (e.g., \"better\" to \"good\").\n",
    "   - **Stopwords Removal**: Remove common words that do not carry significant meaning (e.g., \"and\", \"the\").\n",
    "\n",
    "4. **Text to Vectors**:\n",
    "   - **One-Hot Encoding**: Represent words as binary vectors.\n",
    "   - **Bag of Words (BoW)**: Represent text as a collection of word frequencies.\n",
    "   - **Term Frequency-Inverse Document Frequency (TF-IDF)**: Represent text based on the importance of words in the corpus.\n",
    "   - **Word2Vec**: Represent words as dense vectors based on their context.\n",
    "   - **Average Word2Vec**: Compute the average of word vectors for a text.\n",
    "\n",
    "5. **Machine Learning or Deep Learning Algorithms (Training)**:\n",
    "   - Train a machine learning or deep learning model to classify text as positive, negative, or neutral.\n",
    "   - Common algorithms include Naive Bayes, Support Vector Machines (SVM), Random Forest, and deep learning models like LSTM, GRU, and BERT.\n",
    "\n",
    "### Summary\n",
    "1. **Data Collection**: Gather relevant text data.\n",
    "2. **Text Preprocessing (Part 1)**: Tokenization, lowercase conversion, and regex cleaning.\n",
    "3. **Text Preprocessing (Part 2)**: Stemming, lemmatization, and stopwords removal.\n",
    "4. **Text to Vectors**: Convert text into numerical vectors using techniques like One-Hot Encoding, BoW, TF-IDF, Word2Vec, and Average Word2Vec.\n",
    "5. **Machine Learning or Deep Learning Algorithms (Training)**: Train models to classify sentiment.\n",
    "\n",
    "This structured approach helps in systematically performing sentiment analysis to understand the emotional tone of text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding\n",
    "\n",
    "One-Hot Encoding is a technique used to convert categorical data into a binary (0 or 1) format that can be provided to machine learning algorithms to improve predictions. In the context of text data, one-hot encoding is used to represent words as binary vectors.\n",
    "\n",
    "### Steps to Perform One-Hot Encoding\n",
    "\n",
    "1. **Identify Unique Words**: Identify all unique words in the text corpus.\n",
    "2. **Create Binary Vectors**: Create a binary vector for each word, where the length of the vector is equal to the number of unique words. Each position in the vector corresponds to a unique word, and the position for the word being encoded is set to 1, while all other positions are set to 0.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's illustrate one-hot encoding with a simple example.\n",
    "\n",
    "#### Sample Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love machine learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. **Tokenize the Text**:\n",
    "   ```python\n",
    "   words = text.split()\n",
    "   # Output: ['I', 'love', 'machine', 'learning']\n",
    "   ```\n",
    "\n",
    "2. **Identify Unique Words**:\n",
    "   ```python\n",
    "   unique_words = list(set(words))\n",
    "   # Output: ['I', 'love', 'machine', 'learning']\n",
    "   ```\n",
    "\n",
    "3. **Create One-Hot Encodings**:\n",
    "   ```python\n",
    "   one_hot_encodings = {word: [1 if i == j else 0 for i in range(len(unique_words))] for j, word in enumerate(unique_words)}\n",
    "   # Output: {'I': [1, 0, 0, 0], 'love': [0, 1, 0, 0], 'machine': [0, 0, 1, 0], 'learning': [0, 0, 0, 1]}\n",
    "   ```\n",
    "\n",
    "### Code Implementation\n",
    "Here is a complete code example to perform one-hot encoding on a sample text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"I love machine learning\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = text.split()\n",
    "\n",
    "# Identify unique words\n",
    "unique_words = list(set(words))\n",
    "\n",
    "# Create one-hot encodings\n",
    "one_hot_encodings = {word: [1 if i == j else 0 for i in range(len(unique_words))] for j, word in enumerate(unique_words)}\n",
    "\n",
    "# Print the one-hot encodings\n",
    "for word, encoding in one_hot_encodings.items():\n",
    "    print(f\"{word}: {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I: [1, 0, 0, 0]\n",
    "love: [0, 1, 0, 0]\n",
    "machine: [0, 0, 1, 0]\n",
    "learning: [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Summary\n",
    "- **One-Hot Encoding**: Converts categorical data into binary vectors.\n",
    "- **Steps**: Identify unique words, create binary vectors.\n",
    "- **Usage**: Useful for representing words in a format suitable for machine learning algorithms.\n",
    "\n",
    "One-hot encoding is a simple yet effective way to represent categorical data, including text, in a numerical format that can be used by machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages of One-Hot Encoding\n",
    "\n",
    "#### Advantages\n",
    "1. **Simplicity**:\n",
    "   - One-hot encoding is straightforward to implement and understand.\n",
    "   - It converts categorical data into a binary format that is easy to work with.\n",
    "\n",
    "2. **No Ordinal Relationships**:\n",
    "   - One-hot encoding does not assume any ordinal relationship between categories, making it suitable for nominal data where categories are unordered.\n",
    "\n",
    "3. **Compatibility with Machine Learning Algorithms**:\n",
    "   - Many machine learning algorithms, such as linear regression, logistic regression, and neural networks, require numerical input. One-hot encoding provides a way to convert categorical data into a numerical format.\n",
    "\n",
    "4. **Avoids Bias**:\n",
    "   - By representing each category as a separate binary feature, one-hot encoding avoids introducing bias that could occur if categories were assigned arbitrary numerical values.\n",
    "\n",
    "#### Disadvantages\n",
    "1. **High Dimensionality**:\n",
    "   - One-hot encoding can lead to a significant increase in the dimensionality of the dataset, especially when dealing with categorical features with many unique values. This can result in a sparse matrix and increased computational complexity.\n",
    "\n",
    "2. **Memory Inefficiency**:\n",
    "   - The resulting binary vectors can be memory-inefficient, particularly for large datasets with many categories. Each unique category adds a new dimension, leading to a large number of zeros in the encoded vectors.\n",
    "\n",
    "3. **Curse of Dimensionality**:\n",
    "   - High-dimensional data can suffer from the curse of dimensionality, where the performance of machine learning algorithms degrades due to the sparsity of the data.\n",
    "\n",
    "4. **Scalability Issues**:\n",
    "   - One-hot encoding may not scale well with very large datasets or features with a high cardinality (many unique values), making it impractical for some applications.\n",
    "\n",
    "### Summary\n",
    "- **Advantages**:\n",
    "  - Simple to implement and understand.\n",
    "  - Suitable for nominal data with no ordinal relationships.\n",
    "  - Compatible with many machine learning algorithms.\n",
    "  - Avoids bias from arbitrary numerical assignments.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Can lead to high dimensionality and sparse matrices.\n",
    "  - Memory-inefficient for large datasets.\n",
    "  - May suffer from the curse of dimensionality.\n",
    "  - Scalability issues with high-cardinality features.\n",
    "\n",
    "One-hot encoding is a powerful tool for converting categorical data into a numerical format, but it is essential to consider its limitations and potential impact on the performance and scalability of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)\n",
    "\n",
    "The Bag of Words (BoW) model is a popular technique used in natural language processing (NLP) to represent text data. It converts text into numerical features by counting the occurrences of each word in a document, disregarding grammar and word order but keeping multiplicity.\n",
    "\n",
    "### Steps to Create a Bag of Words Model\n",
    "\n",
    "1. **Text Preprocessing**:\n",
    "   - Tokenize the text into individual words.\n",
    "   - Convert all words to lowercase.\n",
    "   - Remove punctuation and special characters.\n",
    "   - Optionally, remove stopwords and apply stemming or lemmatization.\n",
    "\n",
    "2. **Create Vocabulary**:\n",
    "   - Identify all unique words in the corpus to create a vocabulary.\n",
    "\n",
    "3. **Vector Representation**:\n",
    "   - Create a vector for each document, where each element of the vector represents the count of a word from the vocabulary in that document.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's illustrate the Bag of Words model with a simple example.\n",
    "\n",
    "#### Sample Texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is great\",\n",
    "    \"I love coding in Python\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. **Text Preprocessing**:\n",
    "   ```python\n",
    "   import re\n",
    "   from nltk.corpus import stopwords\n",
    "   from nltk.tokenize import word_tokenize\n",
    "\n",
    "   # Download necessary NLTK data\n",
    "   nltk.download('punkt')\n",
    "   nltk.download('stopwords')\n",
    "\n",
    "   stop_words = set(stopwords.words('english'))\n",
    "\n",
    "   def preprocess(text):\n",
    "       # Lowercase the text\n",
    "       text = text.lower()\n",
    "       # Remove punctuation and special characters\n",
    "       text = re.sub(r'\\W', ' ', text)\n",
    "       # Tokenize the text\n",
    "       words = word_tokenize(text)\n",
    "       # Remove stopwords\n",
    "       words = [word for word in words if word not in stop_words]\n",
    "       return words\n",
    "\n",
    "   preprocessed_docs = [preprocess(doc) for doc in documents]\n",
    "   # Output: [['love', 'machine', 'learning'], ['machine', 'learning', 'great'], ['love', 'coding', 'python']]\n",
    "   ```\n",
    "\n",
    "2. **Create Vocabulary**:\n",
    "   ```python\n",
    "   from collections import Counter\n",
    "\n",
    "   # Flatten the list of preprocessed documents\n",
    "   all_words = [word for doc in preprocessed_docs for word in doc]\n",
    "   # Create a vocabulary of unique words\n",
    "   vocabulary = list(set(all_words))\n",
    "   # Output: ['python', 'coding', 'machine', 'learning', 'love', 'great']\n",
    "   ```\n",
    "\n",
    "3. **Vector Representation**:\n",
    "   ```python\n",
    "   def vectorize(doc, vocabulary):\n",
    "       word_count = Counter(doc)\n",
    "       return [word_count[word] if word in word_count else 0 for word in vocabulary]\n",
    "\n",
    "   vectors = [vectorize(doc, vocabulary) for doc in preprocessed_docs]\n",
    "   # Output: [[0, 0, 1, 1, 1, 0], [0, 0, 1, 1, 0, 1], [1, 1, 0, 0, 1, 0]]\n",
    "   ```\n",
    "\n",
    "### Code Implementation\n",
    "Here is a complete code example to create a Bag of Words model for the sample texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample texts\n",
    "documents = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is great\",\n",
    "    \"I love coding in Python\"\n",
    "]\n",
    "\n",
    "# Preprocess the text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "preprocessed_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Create vocabulary\n",
    "all_words = [word for doc in preprocessed_docs for word in doc]\n",
    "vocabulary = list(set(all_words))\n",
    "\n",
    "# Vectorize documents\n",
    "def vectorize(doc, vocabulary):\n",
    "    word_count = Counter(doc)\n",
    "    return [word_count[word] if word in word_count else 0 for word in vocabulary]\n",
    "\n",
    "vectors = [vectorize(doc, vocabulary) for doc in preprocessed_docs]\n",
    "\n",
    "# Print the vectors\n",
    "for doc, vector in zip(documents, vectors):\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(f\"Vector: {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Document: I love machine learning\n",
    "Vector: [0, 0, 1, 1, 1, 0]\n",
    "Document: Machine learning is great\n",
    "Vector: [0, 0, 1, 1, 0, 1]\n",
    "Document: I love coding in Python\n",
    "Vector: [1, 1, 0, 0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Summary\n",
    "- **Bag of Words (BoW)**: Converts text into numerical features by counting word occurrences.\n",
    "- **Steps**: Text preprocessing, create vocabulary, vector representation.\n",
    "- **Usage**: Useful for text classification, clustering, and other NLP tasks.\n",
    "\n",
    "The Bag of Words model is a simple yet effective way to represent text data in a numerical format suitable for machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages of Bag of Words (BoW)\n",
    "\n",
    "#### Advantages\n",
    "1. **Simplicity**:\n",
    "   - The BoW model is straightforward to implement and understand.\n",
    "   - It converts text into a numerical format that is easy to work with for many machine learning algorithms.\n",
    "\n",
    "2. **Effectiveness**:\n",
    "   - Despite its simplicity, BoW can be quite effective for many text classification tasks, such as spam detection, sentiment analysis, and topic classification.\n",
    "\n",
    "3. **No Need for Linguistic Knowledge**:\n",
    "   - BoW does not require any linguistic knowledge or complex preprocessing steps, making it accessible for a wide range of applications.\n",
    "\n",
    "4. **Flexibility**:\n",
    "   - The model can be easily extended to include n-grams (combinations of words) to capture some context and improve performance.\n",
    "\n",
    "#### Disadvantages\n",
    "1. **High Dimensionality**:\n",
    "   - BoW can lead to a significant increase in the dimensionality of the dataset, especially when dealing with large vocabularies. This can result in a sparse matrix and increased computational complexity.\n",
    "\n",
    "2. **Loss of Context**:\n",
    "   - BoW disregards the order of words and their context within the text. This can lead to a loss of important semantic information.\n",
    "\n",
    "3. **Memory Inefficiency**:\n",
    "   - The resulting vectors can be memory-inefficient, particularly for large datasets with many unique words. Each unique word adds a new dimension, leading to a large number of zeros in the encoded vectors.\n",
    "\n",
    "4. **Curse of Dimensionality**:\n",
    "   - High-dimensional data can suffer from the curse of dimensionality, where the performance of machine learning algorithms degrades due to the sparsity of the data.\n",
    "\n",
    "5. **Scalability Issues**:\n",
    "   - BoW may not scale well with very large datasets or features with a high cardinality (many unique words), making it impractical for some applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words using `CountVectorizer` from `sklearn`\n",
    "\n",
    "The `CountVectorizer` class in `sklearn` is used to convert a collection of text documents to a matrix of token counts. This is a simple and effective way to perform the Bag of Words (BoW) transformation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Features Available in `CountVectorizer`\n",
    "\n",
    "1. **`analyzer`**:\n",
    "   - Determines whether the feature should be made of word or character n-grams.\n",
    "   - Options: `'word'`, `'char'`, `'char_wb'`.\n",
    "\n",
    "2. **`binary`**:\n",
    "   - If `True`, all non-zero counts are set to 1.\n",
    "   - Default: `False`.\n",
    "\n",
    "3. **`decode_error`**:\n",
    "   - Specifies what to do when a byte sequence is given to analyze that contains characters not of the given encoding.\n",
    "   - Options: `'strict'`, `'ignore'`, `'replace'`.\n",
    "   - Default: `'strict'`.\n",
    "\n",
    "4. **`dtype`**:\n",
    "   - Type of the matrix returned by `fit_transform` or `transform`.\n",
    "   - Default: `np.int64`.\n",
    "\n",
    "5. **`encoding`**:\n",
    "   - Character encoding to use.\n",
    "   - Default: `'utf-8'`.\n",
    "\n",
    "6. **`input`**:\n",
    "   - Specifies the input type.\n",
    "   - Options: `'filename'`, `'file'`, `'content'`.\n",
    "   - Default: `'content'`.\n",
    "\n",
    "7. **`lowercase`**:\n",
    "   - Convert all characters to lowercase before tokenizing.\n",
    "   - Default: `True`.\n",
    "\n",
    "8. **`max_df`**:\n",
    "   - Ignore terms that have a document frequency strictly higher than the given threshold.\n",
    "   - Can be an integer (absolute counts) or a float (proportion of documents).\n",
    "   - Default: `1.0`.\n",
    "\n",
    "9. **`min_df`**:\n",
    "   - Ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "   - Can be an integer (absolute counts) or a float (proportion of documents).\n",
    "   - Default: `1`.\n",
    "\n",
    "10. **`ngram_range`**:\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "    - Default: `(1, 1)`.\n",
    "\n",
    "11. **`preprocessor`**:\n",
    "    - Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps.\n",
    "    - Default: `None`.\n",
    "\n",
    "12. **`stop_words`**:\n",
    "    - Remove stop words from the text.\n",
    "    - Options: `'english'`, list of stop words, or `None`.\n",
    "    - Default: `None`.\n",
    "\n",
    "13. **`strip_accents`**:\n",
    "    - Remove accents and perform other character normalization during the preprocessing step.\n",
    "    - Options: `'ascii'`, `'unicode'`, or `None`.\n",
    "    - Default: `None`.\n",
    "\n",
    "14. **`token_pattern`**:\n",
    "    - Regular expression denoting what constitutes a \"token\".\n",
    "    - Default: `r'(?u)\\b\\w\\w+\\b'`.\n",
    "\n",
    "15. **`tokenizer`**:\n",
    "    - Override the string tokenization step while preserving the preprocessing and n-grams generation steps.\n",
    "    - Default: `None`.\n",
    "\n",
    "16. **`vocabulary`**:\n",
    "    - A mapping of terms to feature indices or a list of terms.\n",
    "    - Default: `None`.\n",
    "\n",
    "### Example with Custom Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is great\",\n",
    "    \"I love coding in Python\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer with custom parameters\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print(\"Document-Term Matrix:\\n\", X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of n-grams in [`sklearn`](command:_github.copilot.openSymbolFromReferences?%5B%22sklearn%22%2C%5B%7B%22uri%22%3A%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22x%3A%5C%5CGen_Ai%5C%5C10-Machine%20Learning%20for%20NLP%5C%5CNLP.ipynb%22%2C%22_sep%22%3A1%2C%22external%22%3A%22vscode-notebook-cell%3A%2Fx%253A%2FGen_Ai%2F10-Machine%2520Learning%2520for%2520NLP%2FNLP.ipynb%23X60sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2Fx%3A%2FGen_Ai%2F10-Machine%20Learning%20for%20NLP%2FNLP.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X60sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A0%2C%22character%22%3A5%7D%7D%5D%5D \"Go to definition\")\n",
    "\n",
    "#### n-grams\n",
    "n-grams are contiguous sequences of [`n`](command:_github.copilot.openSymbolFromReferences?%5B%22n%22%2C%5B%7B%22uri%22%3A%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22x%3A%5C%5CGen_Ai%5C%5C10-Machine%20Learning%20for%20NLP%5C%5CNLP.ipynb%22%2C%22_sep%22%3A1%2C%22external%22%3A%22vscode-notebook-cell%3A%2Fx%253A%2FGen_Ai%2F10-Machine%2520Learning%2520for%2520NLP%2FNLP.ipynb%23X60sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2Fx%3A%2FGen_Ai%2F10-Machine%20Learning%20for%20NLP%2FNLP.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X60sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A22%2C%22character%22%3A29%7D%7D%5D%5D \"Go to definition\") items from a given sample of text or speech. In the context of text processing, these items are typically words or characters. n-grams are used to capture the context and structure of the text.\n",
    "\n",
    "#### Types of n-grams\n",
    "1. **Unigram**:\n",
    "   - A unigram is a single word or token.\n",
    "   - Example: For the sentence \"I love machine learning\", the unigrams are [\"I\", \"love\", \"machine\", \"learning\"].\n",
    "\n",
    "2. **Bigram**:\n",
    "   - A bigram is a sequence of two adjacent words or tokens.\n",
    "   - Example: For the sentence \"I love machine learning\", the bigrams are [\"I love\", \"love machine\", \"machine learning\"].\n",
    "\n",
    "3. **Trigram**:\n",
    "   - A trigram is a sequence of three adjacent words or tokens.\n",
    "   - Example: For the sentence \"I love machine learning\", the trigrams are [\"I love machine\", \"love machine learning\"].\n",
    "\n",
    "#### Usage in [`sklearn`](command:_github.copilot.openSymbolFromReferences?%5B%22sklearn%22%2C%5B%7B%22uri%22%3A%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22x%3A%5C%5CGen_Ai%5C%5C10-Machine%20Learning%20for%20NLP%5C%5CNLP.ipynb%22%2C%22_sep%22%3A1%2C%22external%22%3A%22vscode-notebook-cell%3A%2Fx%253A%2FGen_Ai%2F10-Machine%2520Learning%2520for%2520NLP%2FNLP.ipynb%23X60sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2Fx%3A%2FGen_Ai%2F10-Machine%20Learning%20for%20NLP%2FNLP.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X60sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A0%2C%22character%22%3A5%7D%7D%5D%5D \"Go to definition\")\n",
    "In [`sklearn`](command:_github.copilot.openSymbolFromReferences?%5B%22sklearn%22%2C%5B%7B%22uri%22%3A%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22x%3A%5C%5CGen_Ai%5C%5C10-Machine%20Learning%20for%20NLP%5C%5CNLP.ipynb%22%2C%22_sep%22%3A1%2C%22external%22%3A%22vscode-notebook-cell%3A%2Fx%253A%2FGen_Ai%2F10-Machine%2520Learning%2520for%2520NLP%2FNLP.ipynb%23X60sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2Fx%3A%2FGen_Ai%2F10-Machine%20Learning%20for%20NLP%2FNLP.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X60sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A0%2C%22character%22%3A5%7D%7D%5D%5D \"Go to definition\"), the [`CountVectorizer`](command:_github.copilot.openSymbolFromReferences?%5B%22CountVectorizer%22%2C%5B%7B%22uri%22%3A%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22x%3A%5C%5CGen_Ai%5C%5C10-Machine%20Learning%20for%20NLP%5C%5CNLP.ipynb%22%2C%22_sep%22%3A1%2C%22external%22%3A%22vscode-notebook-cell%3A%2Fx%253A%2FGen_Ai%2F10-Machine%2520Learning%2520for%2520NLP%2FNLP.ipynb%23X60sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2Fx%3A%2FGen_Ai%2F10-Machine%20Learning%20for%20NLP%2FNLP.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X60sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A0%2C%22character%22%3A44%7D%7D%5D%5D \"Go to definition\") and `TfidfVectorizer` classes can be configured to extract n-grams from the text data. The [`ngram_range`](command:_github.copilot.openSymbolFromReferences?%5B%22ngram_range%22%2C%5B%7B%22uri%22%3A%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22x%3A%5C%5CGen_Ai%5C%5C10-Machine%20Learning%20for%20NLP%5C%5CNLP.ipynb%22%2C%22_sep%22%3A1%2C%22external%22%3A%22vscode-notebook-cell%3A%2Fx%253A%2FGen_Ai%2F10-Machine%2520Learning%2520for%2520NLP%2FNLP.ipynb%23X60sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2Fx%3A%2FGen_Ai%2F10-Machine%20Learning%20for%20NLP%2FNLP.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X60sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A10%2C%22character%22%3A67%7D%7D%5D%5D \"Go to definition\") parameter is used to specify the range of n-values for different n-grams to be extracted.\n",
    "\n",
    "- **Unigram**: [`ngram_range=(1, 1)`](command:_github.copilot.openSymbolFromReferences?%5B%22ngram_range%3D(1%2C%201)%22%2C%5B%7B%22uri%22%3A%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22x%3A%5C%5CGen_Ai%5C%5C10-Machine%20Learning%20for%20NLP%5C%5CNLP.ipynb%22%2C%22_sep%22%3A1%2C%22external%22%3A%22vscode-notebook-cell%3A%2Fx%253A%2FGen_Ai%2F10-Machine%2520Learning%2520for%2520NLP%2FNLP.ipynb%23X60sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2Fx%3A%2FGen_Ai%2F10-Machine%20Learning%20for%20NLP%2FNLP.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X60sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A10%2C%22character%22%3A67%7D%7D%5D%5D \"Go to definition\")\n",
    "- **Bigram**: [`ngram_range=(2, 2)`](command:_github.copilot.openSymbolFromReferences?%5B%22ngram_range%3D(2%2C%202)%22%2C%5B%7B%22uri%22%3A%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22x%3A%5C%5CGen_Ai%5C%5C10-Machine%20Learning%20for%20NLP%5C%5CNLP.ipynb%22%2C%22_sep%22%3A1%2C%22external%22%3A%22vscode-notebook-cell%3A%2Fx%253A%2FGen_Ai%2F10-Machine%2520Learning%2520for%2520NLP%2FNLP.ipynb%23X60sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2Fx%3A%2FGen_Ai%2F10-Machine%20Learning%20for%20NLP%2FNLP.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X60sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A10%2C%22character%22%3A67%7D%7D%5D%5D \"Go to definition\")\n",
    "- **Trigram**: [`ngram_range=(3, 3)`](command:_github.copilot.openSymbolFromReferences?%5B%22ngram_range%3D(3%2C%203)%22%2C%5B%7B%22uri%22%3A%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22x%3A%5C%5CGen_Ai%5C%5C10-Machine%20Learning%20for%20NLP%5C%5CNLP.ipynb%22%2C%22_sep%22%3A1%2C%22external%22%3A%22vscode-notebook-cell%3A%2Fx%253A%2FGen_Ai%2F10-Machine%2520Learning%2520for%2520NLP%2FNLP.ipynb%23X60sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2Fx%3A%2FGen_Ai%2F10-Machine%20Learning%20for%20NLP%2FNLP.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X60sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A10%2C%22character%22%3A67%7D%7D%5D%5D \"Go to definition\")\n",
    "- **Combination**: [`ngram_range=(1, 2)`](command:_github.copilot.openSymbolFromReferences?%5B%22ngram_range%3D(1%2C%202)%22%2C%5B%7B%22uri%22%3A%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22x%3A%5C%5CGen_Ai%5C%5C10-Machine%20Learning%20for%20NLP%5C%5CNLP.ipynb%22%2C%22_sep%22%3A1%2C%22external%22%3A%22vscode-notebook-cell%3A%2Fx%253A%2FGen_Ai%2F10-Machine%2520Learning%2520for%2520NLP%2FNLP.ipynb%23X60sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2Fx%3A%2FGen_Ai%2F10-Machine%20Learning%20for%20NLP%2FNLP.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X60sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A10%2C%22character%22%3A67%7D%7D%5D%5D \"Go to definition\") (extracts both unigrams and bigrams)\n",
    "\n",
    "By adjusting the [`ngram_range`](command:_github.copilot.openSymbolFromReferences?%5B%22ngram_range%22%2C%5B%7B%22uri%22%3A%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22x%3A%5C%5CGen_Ai%5C%5C10-Machine%20Learning%20for%20NLP%5C%5CNLP.ipynb%22%2C%22_sep%22%3A1%2C%22external%22%3A%22vscode-notebook-cell%3A%2Fx%253A%2FGen_Ai%2F10-Machine%2520Learning%2520for%2520NLP%2FNLP.ipynb%23X60sZmlsZQ%253D%253D%22%2C%22path%22%3A%22%2Fx%3A%2FGen_Ai%2F10-Machine%20Learning%20for%20NLP%2FNLP.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X60sZmlsZQ%3D%3D%22%7D%2C%22pos%22%3A%7B%22line%22%3A10%2C%22character%22%3A67%7D%7D%5D%5D \"Go to definition\") parameter, you can capture different levels of context and structure in the text data, which can be useful for various natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "#### Term Frequency (TF)\n",
    "- **Definition**: Measures how frequently a term appears in a document.\n",
    "- **Formula**: \n",
    "  \\[\n",
    "  \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
    "  \\]\n",
    "- **Purpose**: Captures the importance of a term within a specific document.\n",
    "\n",
    "#### Inverse Document Frequency (IDF)\n",
    "- **Definition**: Measures how important a term is across the entire corpus.\n",
    "- **Formula**: \n",
    "  \\[\n",
    "  \\text{IDF}(t, D) = \\log \\left( \\frac{\\text{Total number of documents } N}{\\text{Number of documents containing term } t} \\right)\n",
    "  \\]\n",
    "- **Purpose**: Reduces the weight of terms that appear frequently across many documents, emphasizing terms that are more unique to specific documents.\n",
    "\n",
    "#### TF-IDF Score\n",
    "- **Definition**: Combines TF and IDF to give a composite score that reflects the importance of a term in a document relative to the entire corpus.\n",
    "- **Formula**: \n",
    "  \\[\n",
    "  \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "  \\]\n",
    "- **Purpose**: Balances the term frequency within a document with the inverse document frequency across the corpus, highlighting terms that are both frequent in a document and rare across the corpus.\n",
    "\n",
    "### Advantages of TF-IDF\n",
    "1. **Relevance**:\n",
    "   - Highlights important terms that are unique to specific documents, improving the relevance of features for tasks like text classification and information retrieval.\n",
    "\n",
    "2. **Simplicity**:\n",
    "   - Easy to understand and implement, making it a popular choice for many text processing applications.\n",
    "\n",
    "3. **Effectiveness**:\n",
    "   - Often provides better results than simple term frequency counts by reducing the impact of common words that are less informative.\n",
    "\n",
    "4. **Normalization**:\n",
    "   - The IDF component helps normalize the term frequency, reducing the bias towards longer documents.\n",
    "\n",
    "### Disadvantages of TF-IDF\n",
    "1. **Sparsity**:\n",
    "   - The resulting vectors can be sparse, especially for large vocabularies, leading to high-dimensional data that can be computationally expensive to process.\n",
    "\n",
    "2. **Context Ignorance**:\n",
    "   - TF-IDF does not capture the semantic meaning or context of terms, potentially missing nuances in the text.\n",
    "\n",
    "3. **Static Nature**:\n",
    "   - The IDF component is static and does not adapt to changes in the corpus over time, which can be a limitation for dynamic datasets.\n",
    "\n",
    "4. **Scalability**:\n",
    "   - Calculating IDF for very large corpora can be computationally intensive, making it less suitable for extremely large datasets without optimization.\n",
    "\n",
    "5. **Sensitivity to Rare Terms**:\n",
    "   - While TF-IDF reduces the weight of common terms, it can sometimes overemphasize rare terms that may not be relevant.\n",
    "\n",
    "\n",
    "TF-IDF is a powerful and widely-used technique for text representation that balances term frequency with inverse document frequency to highlight important terms. However, it has limitations related to sparsity, context ignorance, and scalability that should be considered when applying it to large or dynamic datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "#### Definition\n",
    "Word embeddings are dense vector representations of words that capture their meanings, semantic relationships, and syntactic properties. Unlike traditional methods like Bag of Words or TF-IDF, which produce sparse and high-dimensional vectors, word embeddings create low-dimensional, continuous-valued vectors.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Dense Vectors**:\n",
    "   - Word embeddings represent words as dense vectors in a continuous vector space, typically with dimensions ranging from 50 to 300.\n",
    "\n",
    "2. **Semantic Similarity**:\n",
    "   - Words with similar meanings are located close to each other in the vector space. For example, the vectors for \"king\" and \"queen\" will be closer than the vectors for \"king\" and \"apple\".\n",
    "\n",
    "3. **Contextual Information**:\n",
    "   - Word embeddings capture the context in which words appear, allowing them to encode semantic relationships and syntactic properties.\n",
    "\n",
    "#### Popular Word Embedding Models\n",
    "\n",
    "1. **Word2Vec**:\n",
    "   - Developed by Google, Word2Vec uses neural networks to learn word embeddings. It has two main architectures: Continuous Bag of Words (CBOW) and Skip-gram.\n",
    "   - **CBOW**: Predicts the target word from its context words.\n",
    "   - **Skip-gram**: Predicts the context words from the target word.\n",
    "\n",
    "2. **GloVe (Global Vectors for Word Representation)**:\n",
    "   - Developed by Stanford, GloVe is based on matrix factorization techniques. It constructs a co-occurrence matrix and then factorizes it to obtain word vectors.\n",
    "   - Focuses on capturing global statistical information from the corpus.\n",
    "\n",
    "3. **FastText**:\n",
    "   - Developed by Facebook, FastText extends Word2Vec by representing words as bags of character n-grams. This allows it to handle out-of-vocabulary words and capture subword information.\n",
    "\n",
    "4. **BERT (Bidirectional Encoder Representations from Transformers)**:\n",
    "   - Developed by Google, BERT is a transformer-based model that generates contextualized word embeddings. Unlike static embeddings, BERT produces different embeddings for the same word depending on its context.\n",
    "\n",
    "#### Advantages of Word Embeddings\n",
    "\n",
    "1. **Semantic Richness**:\n",
    "   - Capture semantic relationships and syntactic properties, making them more informative than traditional methods.\n",
    "\n",
    "2. **Dimensionality Reduction**:\n",
    "   - Produce low-dimensional vectors, reducing computational complexity and memory usage.\n",
    "\n",
    "3. **Transfer Learning**:\n",
    "   - Pre-trained word embeddings can be used across different tasks and domains, reducing the need for large labeled datasets.\n",
    "\n",
    "4. **Improved Performance**:\n",
    "   - Enhance the performance of various NLP tasks, such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "#### Disadvantages of Word Embeddings\n",
    "\n",
    "1. **Training Complexity**:\n",
    "   - Training word embeddings requires significant computational resources and large corpora.\n",
    "\n",
    "2. **Static Nature**:\n",
    "   - Traditional word embeddings like Word2Vec and GloVe are static and do not capture the dynamic nature of word meanings in different contexts. Contextual embeddings like BERT address this issue but are more complex.\n",
    "\n",
    "3. **Bias**:\n",
    "   - Word embeddings can capture and propagate biases present in the training data, leading to ethical concerns.\n",
    "\n",
    "4. **Out-of-Vocabulary Words**:\n",
    "   - Static embeddings struggle with out-of-vocabulary words, although models like FastText mitigate this issue by using subword information.\n",
    "\n",
    "\n",
    "Word embeddings are a powerful tool in natural language processing, providing dense, semantically rich vector representations of words. They improve the performance of various NLP tasks by capturing the meanings and relationships of words. However, they come with challenges such as training complexity, potential biases, and handling out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec CBOW(Continuous Bag of Words)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- ANN(Artificial Neural Network)\n",
    "- Loss Function \n",
    "- Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1. Artificial Neural Network (ANN)\n",
    "- **Definition**: ANNs are computational models inspired by the human brain, consisting of interconnected nodes (neurons) organized in layers.\n",
    "- **Components**:\n",
    "  - **Input Layer**: Receives input data.\n",
    "  - **Hidden Layers**: Perform computations and feature extraction.\n",
    "  - **Output Layer**: Produces the final output.\n",
    "- **Training**: Involves adjusting weights using backpropagation to minimize the error between predicted and actual outputs.\n",
    "\n",
    "#### 2. Loss Function\n",
    "- **Definition**: A loss function measures the difference between the predicted output and the actual output.\n",
    "- **Purpose**: Guides the optimization process by providing a metric to minimize.\n",
    "- **Common Loss Functions**:\n",
    "  - **Mean Squared Error (MSE)**: Used for regression tasks.\n",
    "  - **Cross-Entropy Loss**: Used for classification tasks.\n",
    "- **In Word2Vec**: The loss function helps in adjusting the weights to improve the prediction of context words.\n",
    "\n",
    "#### 3. Optimizers\n",
    "- **Definition**: Algorithms used to update the weights of the neural network to minimize the loss function.\n",
    "- **Common Optimizers**:\n",
    "  - **Stochastic Gradient Descent (SGD)**: Updates weights using the gradient of the loss function.\n",
    "  - **Adam (Adaptive Moment Estimation)**: Combines the advantages of two other extensions of SGD, AdaGrad and RMSProp.\n",
    "- **In Word2Vec**: Optimizers adjust the weights of the neural network to improve the accuracy of word predictions.\n",
    "\n",
    "### Summary\n",
    "Understanding ANNs, loss functions, and optimizers is crucial for grasping how Word2Vec CBOW works. ANNs provide the framework, loss functions guide the training process, and optimizers adjust the weights to minimize errors, enabling the model to learn meaningful word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW (Continuous Bag of Words) Model\n",
    "\n",
    "#### Definition\n",
    "The Continuous Bag of Words (CBOW) model is one of the two main architectures used in Word2Vec for learning word embeddings. It aims to predict a target word based on its surrounding context words.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Target Word and Context Words**:\n",
    "   - **Target Word**: The word to be predicted.\n",
    "   - **Context Words**: The surrounding words within a specified window size.\n",
    "\n",
    "2. **Window Size**:\n",
    "   - Defines the number of context words to consider on either side of the target word.\n",
    "   - Example: For a window size of 2, the context words for the target word \"sits\" in the sentence \"The cat sits on the mat\" are [\"The\", \"cat\", \"on\", \"the\"].\n",
    "\n",
    "#### Training Objective\n",
    "\n",
    "- **Objective**: Maximize the probability of the target word given the context words.\n",
    "- **Formula**: \n",
    "  \\[\n",
    "  \\text{Maximize} \\sum_{t=1}^{T} \\log P(w_t | w_{t-c}, \\ldots, w_{t-1}, w_{t+1}, \\ldots, w_{t+c})\n",
    "  \\]\n",
    "  where \\( w_t \\) is the target word, \\( w_{t-c}, \\ldots, w_{t+c} \\) are the context words, \\( c \\) is the window size, and \\( T \\) is the total number of words in the corpus.\n",
    "\n",
    "#### Neural Network Architecture\n",
    "\n",
    "1. **Input Layer**:\n",
    "   - One-hot encoded vectors representing the context words.\n",
    "\n",
    "2. **Hidden Layer**:\n",
    "   - A single hidden layer with a specified number of neurons (embedding size).\n",
    "   - The input vectors are averaged to produce the hidden layer representation.\n",
    "\n",
    "3. **Output Layer**:\n",
    "   - Produces a probability distribution over the vocabulary for the target word.\n",
    "\n",
    "#### Training Process\n",
    "\n",
    "1. **Forward Pass**:\n",
    "   - The input context word vectors are averaged and then multiplied by the weight matrix to produce the hidden layer representation.\n",
    "   - The hidden layer representation is then multiplied by another weight matrix to produce the output probabilities.\n",
    "\n",
    "2. **Loss Calculation**:\n",
    "   - The loss function (typically cross-entropy loss) measures the difference between the predicted and actual target word.\n",
    "\n",
    "3. **Backpropagation**:\n",
    "   - The gradients of the loss function are computed with respect to the weights.\n",
    "   - The weights are updated using an optimizer (e.g., SGD, Adam) to minimize the loss.\n",
    "\n",
    "#### Advantages of CBOW\n",
    "\n",
    "1. **Efficiency**:\n",
    "   - Generally faster to train than the Skip-gram model, especially on large datasets.\n",
    "\n",
    "2. **Semantic Richness**:\n",
    "   - Captures semantic relationships between words, making similar words have similar vector representations.\n",
    "\n",
    "3. **Simplicity**:\n",
    "   - Simpler architecture compared to Skip-gram, making it easier to implement and understand.\n",
    "\n",
    "#### Disadvantages of CBOW\n",
    "\n",
    "1. **Context Averaging**:\n",
    "   - Averaging context words can dilute the importance of individual words, potentially losing some semantic information.\n",
    "\n",
    "2. **Static Embeddings**:\n",
    "   - Produces static embeddings that do not change based on context, limiting their ability to capture polysemy (multiple meanings of a word).\n",
    "\n",
    "3. **Bias**:\n",
    "   - Can capture and propagate biases present in the training data.\n",
    "\n",
    "### Summary\n",
    "The CBOW model in Word2Vec is a powerful technique for learning word embeddings by predicting a target word based on its surrounding context words. It captures semantic relationships between words, producing dense and meaningful vector representations. However, it averages context words, which can dilute individual word importance, and requires substantial computational resources and large datasets for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Skip-gram Model\n",
    "\n",
    "#### Definition\n",
    "The Skip-gram model is one of the two main architectures used in Word2Vec for learning word embeddings. It aims to predict the context words given a target word, capturing the semantic relationships between words.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Target Word and Context Words**:\n",
    "   - **Target Word**: The word for which the context is being predicted.\n",
    "   - **Context Words**: The surrounding words within a specified window size.\n",
    "\n",
    "2. **Window Size**:\n",
    "   - Defines the number of context words to consider on either side of the target word.\n",
    "   - Example: For a window size of 2, the context words for the target word \"sits\" in the sentence \"The cat sits on the mat\" are [\"The\", \"cat\", \"on\", \"the\"].\n",
    "\n",
    "#### Training Objective\n",
    "\n",
    "- **Objective**: Maximize the probability of context words given the target word.\n",
    "- **Formula**: \n",
    "  \\[\n",
    "  \\text{Maximize} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P(w_{t+j} | w_t)\n",
    "  \\]\n",
    "  where \\( w_t \\) is the target word, \\( w_{t+j} \\) are the context words, \\( c \\) is the window size, and \\( T \\) is the total number of words in the corpus.\n",
    "\n",
    "#### Neural Network Architecture\n",
    "\n",
    "1. **Input Layer**:\n",
    "   - One-hot encoded vector representing the target word.\n",
    "\n",
    "2. **Hidden Layer**:\n",
    "   - A single hidden layer with a specified number of neurons (embedding size).\n",
    "\n",
    "3. **Output Layer**:\n",
    "   - Produces a probability distribution over the vocabulary for each context word.\n",
    "\n",
    "#### Training Process\n",
    "\n",
    "1. **Forward Pass**:\n",
    "   - The input word vector is multiplied by the weight matrix to produce the hidden layer representation.\n",
    "   - The hidden layer representation is then multiplied by another weight matrix to produce the output probabilities.\n",
    "\n",
    "2. **Loss Calculation**:\n",
    "   - The loss function (typically cross-entropy loss) measures the difference between the predicted and actual context words.\n",
    "\n",
    "3. **Backpropagation**:\n",
    "   - The gradients of the loss function are computed with respect to the weights.\n",
    "   - The weights are updated using an optimizer (e.g., SGD, Adam) to minimize the loss.\n",
    "\n",
    "#### Advantages of Skip-gram\n",
    "\n",
    "1. **Efficiency**:\n",
    "   - Efficiently handles large datasets and produces high-quality word embeddings.\n",
    "\n",
    "2. **Semantic Richness**:\n",
    "   - Captures semantic relationships between words, making similar words have similar vector representations.\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - Can be used for various NLP tasks such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "#### Disadvantages of Skip-gram\n",
    "\n",
    "1. **Training Complexity**:\n",
    "   - Requires significant computational resources and large corpora for training.\n",
    "\n",
    "2. **Static Embeddings**:\n",
    "   - Produces static embeddings that do not change based on context, limiting their ability to capture polysemy (multiple meanings of a word).\n",
    "\n",
    "3. **Bias**:\n",
    "   - Can capture and propagate biases present in the training data.\n",
    "\n",
    "### Summary\n",
    "The Skip-gram model in Word2Vec is a powerful technique for learning word embeddings by predicting context words given a target word. It captures semantic relationships between words, producing dense and meaningful vector representations. However, it requires substantial computational resources and large datasets for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "#### Definition\n",
    "Gensim is an open-source Python library designed for topic modeling, document indexing, and similarity retrieval with large corpora. It is particularly well-known for its efficient implementation of Word2Vec and other word embedding models.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "1. **Scalability**:\n",
    "   - Designed to handle large text corpora efficiently, using algorithms that scale well with data size.\n",
    "\n",
    "2. **Ease of Use**:\n",
    "   - Provides a simple and intuitive API for training and using word embeddings and other models.\n",
    "\n",
    "3. **Versatility**:\n",
    "   - Supports various models and algorithms, including Word2Vec, FastText, Doc2Vec, LDA (Latent Dirichlet Allocation), and more.\n",
    "\n",
    "4. **Integration**:\n",
    "   - Easily integrates with other Python libraries and tools for natural language processing, such as NLTK and spaCy.\n",
    "\n",
    "#### Installing Gensim\n",
    "\n",
    "To install Gensim, you can use pip:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Using Gensim for Word2Vec\n",
    "\n",
    "Here's a step-by-step guide to training a Word2Vec model using Gensim:\n",
    "\n",
    "1. **Import Libraries**:\n",
    "   - Import the necessary libraries, including Gensim and any preprocessing tools you might need.\n",
    "\n",
    "2. **Prepare Data**:\n",
    "   - Tokenize and preprocess your text data.\n",
    "\n",
    "3. **Train Word2Vec Model**:\n",
    "   - Use Gensim's `Word2Vec` class to train the model.\n",
    "\n",
    "4. **Save and Load Model**:\n",
    "   - Save the trained model to disk and load it for future use.\n",
    "\n",
    "#### Example Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "# Step 2: Prepare Data\n",
    "# Example data: list of tokenized sentences\n",
    "# In practice, replace `common_texts` with your own tokenized text data\n",
    "sentences = common_texts\n",
    "\n",
    "# Step 3: Train Word2Vec Model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 4: Save and Load Model\n",
    "model.save(\"word2vec.model\")\n",
    "loaded_model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Example Usage: Find most similar words\n",
    "similar_words = loaded_model.wv.most_similar(\"computer\")\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Key Parameters for Word2Vec\n",
    "\n",
    "- `vector_size`: Dimensionality of the word vectors.\n",
    "- `window`: Maximum distance between the current and predicted word within a sentence.\n",
    "- `min_count`: Ignores all words with total frequency lower than this.\n",
    "- `workers`: Number of worker threads to train the model.\n",
    "\n",
    "#### Advantages of Gensim\n",
    "\n",
    "1. **Efficiency**:\n",
    "   - Optimized for performance, making it suitable for large datasets.\n",
    "\n",
    "2. **Flexibility**:\n",
    "   - Supports various models and configurations, allowing customization based on specific needs.\n",
    "\n",
    "3. **Community Support**:\n",
    "   - Well-documented with a large user community, providing ample resources and support.\n",
    "\n",
    "#### Disadvantages of Gensim\n",
    "\n",
    "1. **Learning Curve**:\n",
    "   - While the API is intuitive, understanding the underlying concepts and parameters may require some learning.\n",
    "\n",
    "2. **Memory Usage**:\n",
    "   - Training large models can be memory-intensive, requiring sufficient computational resources.\n",
    "\n",
    "### Summary\n",
    "Gensim is a powerful and efficient library for training and using word embeddings and other NLP models. It is particularly well-suited for handling large text corpora and provides a simple API for various tasks. Understanding its key features and parameters can help you effectively leverage Gensim for your NLP projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
