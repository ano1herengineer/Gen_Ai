{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN Forward Propagation with Time\n",
    "\n",
    "In a Simple Recurrent Neural Network (RNN), the forward propagation involves processing sequential data over time steps. Here's a step-by-step explanation of the forward propagation process, including the use of activation functions like softmax or sigmoid.\n",
    "\n",
    "#### Notations\n",
    "- \\( x_t \\): Input at time step \\( t \\)\n",
    "- \\( h_t \\): Hidden state at time step \\( t \\)\n",
    "- \\( y_t \\): Output at time step \\( t \\)\n",
    "- \\( W_x \\): Weight matrix for the input\n",
    "- \\( W_h \\): Weight matrix for the hidden state\n",
    "- \\( b_h \\): Bias for the hidden state\n",
    "- \\( W_y \\): Weight matrix for the output\n",
    "- \\( b_y \\): Bias for the output\n",
    "- \\( \\sigma \\): Activation function (e.g., tanh, ReLU)\n",
    "- \\( \\phi \\): Output activation function (e.g., softmax, sigmoid)\n",
    "\n",
    "#### Forward Propagation Steps\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize the hidden state \\( h_0 \\) (usually set to zeros).\n",
    "\n",
    "2. **Hidden State Calculation**:\n",
    "   - For each time step \\( t \\):\n",
    "     \\[\n",
    "     h_t = \\sigma(W_x x_t + W_h h_{t-1} + b_h)\n",
    "     \\]\n",
    "     where \\( \\sigma \\) is typically the tanh or ReLU activation function.\n",
    "\n",
    "3. **Output Calculation**:\n",
    "   - For each time step \\( t \\):\n",
    "     \\[\n",
    "     o_t = W_y h_t + b_y\n",
    "     \\]\n",
    "     - Apply the output activation function \\( \\phi \\) (e.g., softmax for classification, sigmoid for binary classification):\n",
    "     \\[\n",
    "     y_t = \\phi(o_t)\n",
    "     \\]\n",
    "\n",
    "4. **Loss Calculation**:\n",
    "   - Compute the loss using the predicted output \\( y_t \\) and the actual target \\( \\hat{y}_t \\):\n",
    "     \\[\n",
    "     \\text{loss} = \\text{LossFunction}(y_t, \\hat{y}_t)\n",
    "     \\]\n",
    "     - Common loss functions include Mean Squared Error (MSE) for regression and Cross-Entropy Loss for classification.\n",
    "\n",
    "### Example Code\n",
    "\n",
    "Here's a simple implementation of forward propagation in a Simple RNN using Python and NumPy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.3012652300060026\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Initialize parameters\n",
    "input_size = 3\n",
    "hidden_size = 2\n",
    "output_size = 2\n",
    "time_steps = 5\n",
    "\n",
    "W_x = np.random.randn(hidden_size, input_size)\n",
    "W_h = np.random.randn(hidden_size, hidden_size)\n",
    "b_h = np.zeros((hidden_size, 1))\n",
    "W_y = np.random.randn(output_size, hidden_size)\n",
    "b_y = np.zeros((output_size, 1))\n",
    "\n",
    "# Initialize hidden state\n",
    "h_t = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Example input sequence (time_steps x input_size)\n",
    "x = np.random.randn(time_steps, input_size, 1)\n",
    "\n",
    "# Forward propagation\n",
    "outputs = []\n",
    "for t in range(time_steps):\n",
    "    x_t = x[t]\n",
    "    h_t = tanh(np.dot(W_x, x_t) + np.dot(W_h, h_t) + b_h)\n",
    "    o_t = np.dot(W_y, h_t) + b_y\n",
    "    y_t = softmax(o_t)  # or sigmoid(o_t) for binary classification\n",
    "    outputs.append(y_t)\n",
    "\n",
    "# Example target sequence (time_steps x output_size)\n",
    "y_true = np.random.randint(0, 2, (time_steps, output_size, 1))\n",
    "\n",
    "# Loss calculation (Cross-Entropy Loss for classification)\n",
    "loss = 0\n",
    "for t in range(time_steps):\n",
    "    y_pred = outputs[t]\n",
    "    y_actual = y_true[t]\n",
    "    loss += -np.sum(y_actual * np.log(y_pred))\n",
    "\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Summary\n",
    "In a Simple RNN, forward propagation involves calculating the hidden state and output at each time step using weight matrices, biases, and activation functions. The loss is then computed using the predicted and actual outputs. Activation functions like tanh, sigmoid, and softmax are commonly used to introduce non-linearity and produce the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN Backward Propagation with Time\n",
    "\n",
    "Backward propagation through time (BPTT) is the process of updating the weights in a recurrent neural network (RNN) by computing the gradients of the loss function with respect to the weights. This involves using the chain rule to propagate errors backward through time.\n",
    "\n",
    "#### Notations\n",
    "- \\( x_t \\): Input at time step \\( t \\)\n",
    "- \\( h_t \\): Hidden state at time step \\( t \\)\n",
    "- \\( y_t \\): Output at time step \\( t \\)\n",
    "- \\( W_x \\): Weight matrix for the input\n",
    "- \\( W_h \\): Weight matrix for the hidden state\n",
    "- \\( b_h \\): Bias for the hidden state\n",
    "- \\( W_y \\): Weight matrix for the output\n",
    "- \\( b_y \\): Bias for the output\n",
    "- \\( \\sigma \\): Activation function (e.g., tanh, ReLU)\n",
    "- \\( \\phi \\): Output activation function (e.g., softmax, sigmoid)\n",
    "- \\( \\eta \\): Learning rate\n",
    "\n",
    "### 1. Update \\( W_y \\) (Output Weights)\n",
    "\n",
    "#### Forward Pass\n",
    "1. Compute the hidden state:\n",
    "   \\[\n",
    "   h_t = \\sigma(W_x x_t + W_h h_{t-1} + b_h)\n",
    "   \\]\n",
    "2. Compute the output:\n",
    "   \\[\n",
    "   o_t = W_y h_t + b_y\n",
    "   \\]\n",
    "3. Apply the output activation function:\n",
    "   \\[\n",
    "   y_t = \\phi(o_t)\n",
    "   \\]\n",
    "\n",
    "#### Backward Pass\n",
    "1. Compute the gradient of the loss with respect to the output:\n",
    "   \\[\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial o_t} = y_t - \\hat{y}_t\n",
    "   \\]\n",
    "2. Compute the gradient of the loss with respect to \\( W_y \\):\n",
    "   \\[\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial W_y} = \\frac{\\partial \\text{Loss}}{\\partial o_t} \\cdot h_t^T\n",
    "   \\]\n",
    "3. Update \\( W_y \\):\n",
    "   \\[\n",
    "   W_y^{\\text{new}} = W_y^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial W_y}\n",
    "   \\]\n",
    "\n",
    "### 2. Update \\( W_h \\) (Hidden Layer Weights)\n",
    "\n",
    "#### Backward Pass\n",
    "1. Compute the gradient of the loss with respect to the hidden state:\n",
    "   \\[\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial h_t} = W_y^T \\cdot \\frac{\\partial \\text{Loss}}{\\partial o_t} + W_h^T \\cdot \\frac{\\partial \\text{Loss}}{\\partial h_{t+1}}\n",
    "   \\]\n",
    "2. Compute the gradient of the loss with respect to \\( W_h \\):\n",
    "   \\[\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial W_h} = \\sum_{t=1}^{T} \\frac{\\partial \\text{Loss}}{\\partial h_t} \\cdot h_{t-1}^T\n",
    "   \\]\n",
    "3. Update \\( W_h \\):\n",
    "   \\[\n",
    "   W_h^{\\text{new}} = W_h^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial W_h}\n",
    "   \\]\n",
    "\n",
    "### 3. Update \\( W_x \\) (Input Weights)\n",
    "\n",
    "#### Backward Pass\n",
    "1. Compute the gradient of the loss with respect to the input weights:\n",
    "   \\[\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial W_x} = \\sum_{t=1}^{T} \\frac{\\partial \\text{Loss}}{\\partial h_t} \\cdot x_t^T\n",
    "   \\]\n",
    "2. Update \\( W_x \\):\n",
    "   \\[\n",
    "   W_x^{\\text{new}} = W_x^{\\text{old}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial W_x}\n",
    "   \\]\n",
    "\n",
    "### Example Code\n",
    "\n",
    "Here's a simplified example of backward propagation through time (BPTT) in a Simple RNN using Python and NumPy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated W_x: [[-1.03157121  0.93281897  0.40706512]\n",
      " [-0.25239521 -0.99046712 -0.75626001]]\n",
      "Updated W_h: [[1.54997476 0.86012955]\n",
      " [0.60104305 0.11456934]]\n",
      "Updated W_y: [[ 1.27003144 -0.67645551]\n",
      " [-1.21092456  0.82633031]]\n",
      "Updated b_h: [[-0.00564082]\n",
      " [ 0.0009489 ]]\n",
      "Updated b_y: [[-0.00335428]\n",
      " [ 0.00120039]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "# Initialize parameters\n",
    "input_size = 3\n",
    "hidden_size = 2\n",
    "output_size = 2\n",
    "time_steps = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "W_x = np.random.randn(hidden_size, input_size)\n",
    "W_h = np.random.randn(hidden_size, hidden_size)\n",
    "b_h = np.zeros((hidden_size, 1))\n",
    "W_y = np.random.randn(output_size, hidden_size)\n",
    "b_y = np.zeros((output_size, 1))\n",
    "\n",
    "# Example input sequence (time_steps x input_size)\n",
    "x = np.random.randn(time_steps, input_size, 1)\n",
    "\n",
    "# Example target sequence (time_steps x output_size)\n",
    "y_true = np.random.randint(0, 2, (time_steps, output_size, 1))\n",
    "\n",
    "# Forward propagation\n",
    "h = np.zeros((time_steps, hidden_size, 1))\n",
    "o = np.zeros((time_steps, output_size, 1))\n",
    "y_pred = np.zeros((time_steps, output_size, 1))\n",
    "\n",
    "for t in range(time_steps):\n",
    "    h[t] = tanh(np.dot(W_x, x[t]) + np.dot(W_h, h[t-1]) + b_h)\n",
    "    o[t] = np.dot(W_y, h[t]) + b_y\n",
    "    y_pred[t] = sigmoid(o[t])\n",
    "\n",
    "# Backward propagation through time (BPTT)\n",
    "dW_x = np.zeros_like(W_x)\n",
    "dW_h = np.zeros_like(W_h)\n",
    "dW_y = np.zeros_like(W_y)\n",
    "db_h = np.zeros_like(b_h)\n",
    "db_y = np.zeros_like(b_y)\n",
    "\n",
    "dh_next = np.zeros_like(h[0])\n",
    "\n",
    "for t in reversed(range(time_steps)):\n",
    "    do = y_pred[t] - y_true[t]\n",
    "    dW_y += np.dot(do, h[t].T)\n",
    "    db_y += do\n",
    "    \n",
    "    dh = np.dot(W_y.T, do) + dh_next\n",
    "    dh_raw = tanh_derivative(h[t]) * dh\n",
    "    dW_x += np.dot(dh_raw, x[t].T)\n",
    "    dW_h += np.dot(dh_raw, h[t-1].T)\n",
    "    db_h += dh_raw\n",
    "    \n",
    "    dh_next = np.dot(W_h.T, dh_raw)\n",
    "\n",
    "# Update weights and biases\n",
    "W_x -= learning_rate * dW_x\n",
    "W_h -= learning_rate * dW_h\n",
    "W_y -= learning_rate * dW_y\n",
    "b_h -= learning_rate * db_h\n",
    "b_y -= learning_rate * db_y\n",
    "\n",
    "print(\"Updated W_x:\", W_x)\n",
    "print(\"Updated W_h:\", W_h)\n",
    "print(\"Updated W_y:\", W_y)\n",
    "print(\"Updated b_h:\", b_h)\n",
    "print(\"Updated b_y:\", b_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Summary\n",
    "Backward propagation through time (BPTT) in a Simple RNN involves updating the weights \\( W_x \\), \\( W_h \\), and \\( W_y \\) by computing the gradients of the loss function with respect to these weights. The gradients are computed using the chain rule, and the weights are updated using the gradient descent algorithm. This process allows the RNN to learn from sequential data and improve its predictions over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with RNN and ANN\n",
    "\n",
    "#### 1. Vanishing Gradient Problem\n",
    "- **Definition**: The vanishing gradient problem occurs when the gradients of the loss function become very small during backpropagation, causing the weights to update very slowly or not at all.\n",
    "- **Impact**: This problem makes it difficult for the network to learn long-term dependencies, as the gradients diminish exponentially as they are propagated back through time.\n",
    "\n",
    "#### 2. Long-Term Dependency\n",
    "- **Definition**: Long-term dependency refers to the ability of a neural network to remember information from earlier time steps in a sequence.\n",
    "- **Impact**: Simple RNNs struggle to capture long-term dependencies due to the vanishing gradient problem, making them ineffective for tasks that require understanding context over long sequences.\n",
    "\n",
    "### Solutions to the Problems\n",
    "\n",
    "#### 1. ReLU and Leaky ReLU\n",
    "- **ReLU (Rectified Linear Unit)**:\n",
    "  - **Definition**: An activation function that outputs the input directly if it is positive; otherwise, it outputs zero.\n",
    "  - **Formula**: \\( \\text{ReLU}(x) = \\max(0, x) \\)\n",
    "  - **Advantages**: Helps mitigate the vanishing gradient problem by providing a constant gradient for positive inputs.\n",
    "  - **Disadvantages**: Can suffer from the \"dying ReLU\" problem, where neurons get stuck in the zero state and stop learning.\n",
    "\n",
    "- **Leaky ReLU**:\n",
    "  - **Definition**: A variant of ReLU that allows a small, non-zero gradient when the input is negative.\n",
    "  - **Formula**: \\( \\text{Leaky ReLU}(x) = \\max(0.01x, x) \\)\n",
    "  - **Advantages**: Addresses the \"dying ReLU\" problem by allowing a small gradient for negative inputs.\n",
    "\n",
    "#### 2. LSTM (Long Short-Term Memory)\n",
    "- **Definition**: A type of RNN designed to capture long-term dependencies by using special units called memory cells.\n",
    "- **Components**:\n",
    "  - **Cell State**: Maintains long-term memory.\n",
    "  - **Forget Gate**: Decides what information to discard from the cell state.\n",
    "  - **Input Gate**: Decides what new information to add to the cell state.\n",
    "  - **Output Gate**: Decides what information to output from the cell state.\n",
    "- **Advantages**: Effectively captures long-term dependencies and mitigates the vanishing gradient problem.\n",
    "\n",
    "#### 3. GRU (Gated Recurrent Unit)\n",
    "- **Definition**: A simplified version of LSTM that combines the forget and input gates into a single update gate.\n",
    "- **Components**:\n",
    "  - **Update Gate**: Controls the flow of information to the hidden state.\n",
    "  - **Reset Gate**: Controls the flow of information from the previous hidden state.\n",
    "- **Advantages**: Simpler architecture than LSTM, making it computationally efficient while still capturing long-term dependencies.\n",
    "\n",
    "### Summary of Solutions\n",
    "\n",
    "| Solution       | Description                                                                 | Advantages                                                                                   | Disadvantages                          |\n",
    "|----------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------|\n",
    "| **ReLU**       | Activation function that outputs the input if positive, otherwise zero      | Mitigates vanishing gradient problem, simple to implement                                    | Can suffer from \"dying ReLU\" problem   |\n",
    "| **Leaky ReLU** | Variant of ReLU allowing small gradient for negative inputs                 | Addresses \"dying ReLU\" problem, maintains non-zero gradient for negative inputs              | Slightly more complex than ReLU        |\n",
    "| **LSTM**       | RNN variant with memory cells and gates to capture long-term dependencies   | Effectively captures long-term dependencies, mitigates vanishing gradient problem            | More complex architecture, computationally intensive |\n",
    "| **GRU**        | Simplified LSTM with combined update and reset gates                        | Simpler and more efficient than LSTM, captures long-term dependencies                        | Less expressive than LSTM              |\n",
    "\n",
    "### Example Code for LSTM and GRU in Python using Keras\n",
    "\n",
    "#### LSTM Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(timesteps, input_dim)))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### GRU Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(GRU(50, input_shape=(timesteps, input_dim)))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Summary\n",
    "Simple RNNs and ANNs face challenges like the vanishing gradient problem and difficulty in capturing long-term dependencies. These issues can be addressed using activation functions like ReLU and Leaky ReLU, and advanced RNN architectures like LSTM and GRU. These solutions enable the network to learn more effectively from sequential data and capture long-term dependencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
